{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e1f47",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "Dieses Tutorial ist von mir selbst erstellt. Der Fokus liegt hier nicht auf \"auswendig-Lernen\" oder einfach nur dem Wissen sämtlicher Details von PyTorch-Funktionalitäten, sondern soll ein funktionales Verständnis ermöglichen, d.h. gewisse Dinge werden evtl. erst selbst implementiert (oder schrittweise hergeleitet), bevor sie hier gezeigt werden. Dieses Tutorial umfasst auch mathematische Formeln, und vieles Weiteres. Es soll die wichtigsten Funktionalitäten von PyTorch zeigen. (und diese Funktionalität auch von anderen Frameworks abgrenzen!!!) PyTorch wird immer wieder angepasst, erweitert und ist keine statische Bibliothek!! (ich versuche framework-agnostisch daran zu gehen, das fällt mir aber sehr schwer, da ich ja nocg gar keine anderen frameworks kenne!!)\n",
    "\n",
    "> PyTorch (kurz: Torch) ist eine 2016 erschienene Programmbibliothek, basierend auf der in Lua geschriebenen Bibliothek Torch.\n",
    "\n",
    "Wichtige Zusatzbibliotheken für PyTorch (bes. im Bereich Maschinelles Lernen) sind:\n",
    "* **Bilderkennung**: `torchvision` (images)\n",
    "* *Texterkennung*: `torchtext` \n",
    "* *Sprach-/Audioerkennung*: `torchaudio`\n",
    "\n",
    "Eine grundlegende Referenz ist unter anderem:  \n",
    "- [Offizielle PyTorch-Website](https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386568a",
   "metadata": {},
   "source": [
    "### Voraussetzungen und Einrichtung der Arbeitsumgebung\n",
    "\n",
    "Ein kurzer Hinweis vorab: Für produktives Arbeiten mit PyTorch benötigt es eine korrekt eingerichtete Ausführungsumgebung sowie grundlegende Vertrautheit mit Python. Kenntnisse in **NumPy** sind hilfreich, aber nicht notwendig.\n",
    "\n",
    "### Benötigte Komponenten\n",
    "Zur Einrichtung der Ausführungsumgebung ist es wichtig, folgende Software zu installieren:\n",
    "- **Python** (ich nutze: *Python 3.9.25*)\n",
    "- **pip** (ich nutze *pip 24.0*)\n",
    "- um **Jupyter Notebooks** zu nutzen gibt es mehrere Möglichkeiten, u.a.\n",
    "    - in VSC: Extensions für Jupyter Notebooks installieren, ipykernel package im virtual environment für Python installieren\n",
    "- Letztlich muss NumPy über einen beliebigen package manager (z.B. pip) in das virtual environment installiert werden, in dem man arbeitet. (am besten eine Version im PyPI-Index) (ich nutze torch, version *2.0.0*)\n",
    "\n",
    "Alles andere ist nicht notwendig, um dieses Tutorial zu verstehen. (z.B. Python Linter, Debugging Tools), usw.\n",
    "\n",
    "(evtl. diesen Aspekt erweitern??)\n",
    "\n",
    "---\n",
    "\n",
    "Die Englische Variante findet sich hier: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "886a14f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cu117'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beginnen wir damit, PyTorch erstmal zu importieren.\n",
    "\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55758e7b",
   "metadata": {},
   "source": [
    "> Kurzer Disclaimer: In diesem Tutorial werden wir noch nicht so weit in andere packages vordringen, und eher bei `torch` bleiben, d.h.\n",
    "* Tensor-Klasse, inklusive einiger fundamentaler Klassen bzw. Datentypen von PyTorch\n",
    "* Attribute von Tensoren\n",
    "* verschiedene Funktionem, um Tensoren verschiedener Art zu erstellen\n",
    "* Operationen auf Tensoren (Methoden)\n",
    "* Verschiedenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2313289",
   "metadata": {},
   "source": [
    "# Was sind Tensoren?\n",
    "Ein wesentlicher \"Typ\" von mathematischen Objekten, mit denen sich PyTorch befasst, sind Tensoren.\n",
    "Mathematisch gesehen sind Tensoren die Verallgemeinerung von bereits bekannten Konzepten hin zu einem mathematischen Objekt mit beliebig vielen **räumlichen/strukturellen Dimensionen**.\n",
    "Im Bereich von Data Science, des Maschinellen Lernens und neuronalen Netzen sind Tensoren ein wesentlicher Bestandteil. Sie wurden als mathematisches Objekt quasi \"wiederentdeckt\", um Berechnungen aus der linearen Algebra und Vektoralgebra in höherdimensionalen Räumen zu ermöglichen. (evtl. bessere Motivation??)\n",
    "\n",
    "Betrachten wir zunächst einen einfachen Skalar $$a \\in \\mathbb{R}$$ \n",
    "Man könnte auch schreiben $$a \\in \\mathbb{R}^{1x1}$$ \n",
    "Anders ausgedrückt: Ein Skalar kann auch als Vektor mit nur einem Element verstanden werden. Ein Zeilen- oder Spaltenvektor, der nur ein Element enthält, ist de facto der gleiche Vektor. \n",
    "\n",
    "Sei der Vektor $$u \\in \\mathbb{R}^n$$ Man könnte auch schreiben: $v \\in \\mathbb{R}^{nx1}$ oder $w \\in \\mathbb{R}^{1xn}$, je nachdem, ob es sich um einen Zeilen- odre Spaltenvektor handelt. Sofern gilt, dass $n=1$, gilt $v=w$. Darüber hinaus bleibt die Frage offen, ob nun gilt, dass $v=u$? (Das ist ein anderes Thema und soll in Morphismen einführen.)\n",
    "\n",
    "Der springende Punkt ist: Skalaren lassen sich als Sonderfall von Vektoren auffassen, wenn diese nur ein Element enthalten. Vektoren können je nach Anwendungsgebiet unterschiedlich definiert sein. (Dies hat schon vielen Mathematikern vor den Kopf gestoßen, oder Informatikern, die es etwas genauer nehmen wollen.) Der wesentliche Unterschied liegt darin, ob gilt, dass $v=u$, oder nicht.\n",
    "> Hier eine kurze Aufgabe: Was für Konsequenzen hat das, wenn das gilt bzw. wenn das nicht gilt?\n",
    "\n",
    "Und jetzt gehen wir nochmals weiter: Vektoren lassen sich auch als Spezialfälle von Matrizen ansehen. Somit auch Skalare. Jede Matrix, die in jeweils einer räumlichen Dimension nur ein \"Element\" enthält, beschreibt einen Vektor. Sei $A = (a_{ij})_{i=1,...m; j=1,...n}$ Sofern gilt, dass $m=1 \\vee n=1$, beschreibt die Matrix einen Vektor mit $m$ bzw. $n$ Elementen.\n",
    "\n",
    "Es lässt sich feststellen, dass Skalare $0$ räumliche Dimensionen haben ($a \\in \\mathbb{R}^{nx0x0x0x0x0x0x...}$), Vektoren (klassischerweise) $1$ räumliche Dimension, und Matrizen $2$. Verallgemeinert man dies auf mathematische Strukturen mit beliebig vielen räumlichen/strukturellen Dimensionen $k$, beschreibt man das, was man einen Tensor nennt. (Das Hat Woldemar Voigt 1898 auch bereits erkannt, [[1](https://books.google.de/books?id=JpenEAAAQBAJ)])\n",
    "* Skalare = Tensoren vom Typ $(0,0)$, Tensor 0. Stufe\n",
    "* \"Spaltenvektoren\" = Tensoren vom Typ $(1,0)$, Tensor 1. Stufe\n",
    "* \"Zeilenvektoren\" = Tensoren vom Typ $(0,1)$, Tensor 1. Stufe\n",
    "* Matrizen = Tensoren 2. Stufe\n",
    "* ??\n",
    "\n",
    "\n",
    "Ja, so wirklich verstanden, WAS ein Tensor nun ist, habe ich immer noch nicht.\n",
    "\n",
    "\n",
    "- Indexnotation\n",
    "- Beziehung zu abelschen Gruppen\n",
    "- Tensorprodukte????\n",
    "- kovariant, kontravariant bzgl. einer Stufe\n",
    "- tensoralgebra\n",
    "- tensor = multilineare Abbildung\n",
    "- Tensor = Array beliebiger Ordnung; multilineare Abbildung (und kann auch als Element eines Vektorraums interpretiert werden!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282c48d",
   "metadata": {},
   "source": [
    "PyTorch führt eine neue Klasse bzw. Datentyp ein, nämlich `torch.Tensor`.\n",
    "Auf diese Art & Weise, lassen sich Tensoren deklarieren, initialisieren, deren Attribute verändern und lesen, und gewisse Methoden der Tensoren nutzen. (Ich glaube nicht, dass das wirklich Tensoren sind.)\n",
    "\n",
    "Es gibt mehrere Möglichkeiten, Tensoren zu erstellen. Man könnte einfach den Konstruktor `torch.Tensor(data: Any)` nutzen, um damit Tensoren zu erstellen.\n",
    "Es empfiehlt sich allerdings die Funktion `torch.tensor(data: Any)` zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "595f7c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "tensor([1, 2, 3])\n",
      "tensor([[ 1., -1.],\n",
      "        [ 1., -1.]])\n",
      "tensor([[[1, 0],\n",
      "         [0, 1]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [1, 0]]])\n",
      "tensor([1., 2., 3., 4., 5., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# Wir können erstmal damit anfangen, Tensoren in PyTorch zu deklarieren und zu initialisieren.\n",
    "# Dazu nutzen wir Python-Sequenzen oder Listen\n",
    "# Es existiert die Klasse Tensor. Wir könnten einfach den Konstruktor aufrufen.; es existeir aber auch die Funktion tensor() (für leaf-Tensoren!)\n",
    "# Was macht den Unterschied??\n",
    "\n",
    "scalar = torch.tensor(7)\n",
    "print(scalar)\n",
    "vector = torch.tensor([1,2,3])\n",
    "print(vector)\n",
    "matrix = torch.tensor([[1., -1.], [1., -1.]]) #2x2-Matrix; erst die Zeilen, dann die Spalten!!!!!!!!!!!!!\n",
    "print(matrix)\n",
    "fourDimensionalTensor = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [1,0], [0,1]\n",
    "        ], \n",
    "        [\n",
    "            [0,1], [1,0]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(fourDimensionalTensor)\n",
    "\n",
    "T = torch.Tensor([1,2,3,4,5,5,6,7,8,9])\n",
    "print(T)\n",
    "\n",
    "# Aufgabe: höherdimensionale Tensoren durchdenken!!!\n",
    "# was genau sind leaf-Tensoren?\n",
    "# Wie ist die Reihenfolge der Matrix-Elemente??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58373a36",
   "metadata": {},
   "source": [
    "Wie man sieht macht es sich PyTorch recht einfach. Tensoren sind einfach nur geschachtelte bzw. mehrdimensionale Arrays.\n",
    "Um im Folgenden uns nicht unnötig zu verwirren, sollen Tensoren im folgenden als Spezialfall von Tensoren aufgefasst werden, nämlich Tensoren 1. Stufe bzw mit `shape == torch.Size([n])`. Ignorieren wir einfach mal die Vektoralgebra ein wenig.\n",
    "Vektoren sind also Elemente aus $\\mathbb{R}^{n \\times 1}$ bzw. $\\mathbb{R}^{1 \\times n}$, wobei in manchen Kontexten die Isomorphie zu $\\mathbb{R}^{n}$ genutzt wird.\n",
    "Im Abschnitt \"Operationen auf Tensoren\" werden wir sehen, wieso es sinnvoller ist, Vektorräume zu ignorieren und stattdessen die Matrizenrechnung als Grundlage zu verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c62c3",
   "metadata": {},
   "source": [
    "## Eigenschaften von Tensoren\n",
    "Wichtigste Attribute von `torch.Tensor` sind:\n",
    "* `ndim`: strukturelle Dimensionen eines Tensors (`int`), Ordnung/Rang\n",
    "* `shape`: Angabe eines Arrays, dessen Länge der Ordnung des Tensors entspricht mit jew. der Dimension bzw. Länge der einzelnen Achsen (kann man sich auch mit der Methode `Tensor.size()` ausgeben lassen)\n",
    "* `requires_grad`: Angabe, ob die Gradienten für diesen Tensor berechnet werden sollen oder nicht (`bool`) (wird später benötigt!!) (Standard ist `False`)\n",
    "\n",
    "Attribute mit torch-eigenen Datentypen, die jeder `torch.Tensor` hat sind u.a.:\n",
    "* `dtype`: Objekt, das den Datentyp der Daten eines Tensors beschreibt, u.a. `None`, `torch.float16`/`torch.half`, `torch.float32`/`torch.float`, `torch.float64`/`torch.double`, `torch.int32`, `torch.int64`, `torch.bool`, (es gibt noch viel mehr!) (`torch.dtype`) (Standard ist float32) (ist unveränderbar)\n",
    "* `device`: Objekt, das das Gerät repräsentiert, auf dem der Tensor zugewiesen ist (d.h. `None`, \"cpu, \"cuda\", usw.) (`torch.device`) (Standard ist \"cpu\") (ist unveränderbar)\n",
    "* `layout`: Objekt, das die Speicheranordnung eines Tensors repräsentiert (`torch.layout`) (Standard ist \"strided\")\n",
    "...\n",
    "\n",
    "Hier wird auch direkt klar, dass `PyTorch` ebenfalls die Klasse `torch.Size` nutzt (Unterklasse von `tuple`), um die \"Form\" von Tensoren zu beschreiben. Das Array beschreibt die Achsenlängen jeweils von der \"äußersten\" zur \"innersten\" Dimension. (d.h. `torch.Size([k_i, ..., k_2, k_1])`)\n",
    "\n",
    "(Diese Attribute sind recht nützlich, um sich die Tensoren besser vorzustellen, mit denen man agiert.)\n",
    "(Man sollte regelmäßig shape, dtype und device ÜBERPRÜFEN!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c26821d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(scalar.ndim)\n",
    "print(vector.ndim)\n",
    "print(matrix.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25c386e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([10])\n",
      "1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for tensor in [scalar, vector, matrix]:\n",
    "    print(tensor.shape)\n",
    "\n",
    "s = T.size()\n",
    "print(s)\n",
    "print(len(s))\n",
    "print(s.numel()) #torch.Size.numel()\n",
    "\n",
    "#Was macht den Unterschied aus??; Ausgabe scheint die gleiche zu sein??\n",
    "\n",
    "#erstellen von Tensoren, wo man nur die Größe änder, d.h.\n",
    "#t = T.size(2,3)\n",
    "#t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587ba79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.int64\n",
      "torch.float32\n",
      "----------\n",
      "torch.int64\n",
      "torch.float32\n",
      "torch.bool\n",
      "----------\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "torch.int32\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n",
      "torch.float64\n",
      "tensor([1, 2, 3])\n",
      "torch.int64\n",
      "torch.IntTensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_312180/1343848177.py:17: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  tensor3 = torch.tensor([1., 2., 3.], dtype=torch.int64) #-> wirft DeprecationWarning!\n"
     ]
    }
   ],
   "source": [
    "for tensor in [scalar, vector, matrix]:\n",
    "    print(tensor.dtype)\n",
    "print(10 * \"-\")\n",
    "\n",
    "#Standard-dtypes\n",
    "x_int = torch.tensor([1,2,3])\n",
    "x_float = torch.tensor([[4.3, 5.23], [4.6, 9.2]])\n",
    "x_bool = torch.tensor([[True, False], [True, False]])\n",
    "\n",
    "print(x_int.dtype)\n",
    "print(x_float.dtype)\n",
    "print(x_bool.dtype)\n",
    "\n",
    "#Man kann den dtype i.d.R. angeben bei der Erstellung von Tensoren, z.B.\n",
    "tensor1 = torch.tensor([1,2,3], dtype=torch.int32)\n",
    "tensor2 = torch.tensor([1,2,3], dtype=torch.float64)\n",
    "tensor3 = torch.tensor([1., 2., 3.], dtype=torch.int64) #-> wirft DeprecationWarning!\n",
    "print(10 * \"-\")\n",
    "for tensor in [tensor1, tensor2, tensor3]:\n",
    "    print(tensor)\n",
    "    print(tensor.dtype)\n",
    "#für nicht Standard-Datentypen wird dieser angezeigt!!\n",
    "\n",
    "tensor4 = torch.tensor([1,2,3], dtype=torch.int32)\n",
    "print(tensor4.type()) #gibt tw. Alternativnamen aus; Warum?????????????????\n",
    "\n",
    "#Casting lassen wir mal außen vor; tensor.type() ???\n",
    "\n",
    "#Sollte man überhaupt so auf die Attribute von torch.Tensor zugreifen?, Wieso gibt es tensor.size()? als Methode, aber nicht sowas für die anderen Attribute??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c102494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cuda:0\n",
      "cuda:1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Es gibt verschiedene Arten von Gerätetypen. Die typischten sind: \"cpu\", \"cuda\" (oder auch \"mps\", \"xpu\", \"xla\", \"meta\")\n",
    "# Es gibt auch sowas wie Geräteordnungszahlen (device ordinals) und accelerators!\n",
    "\n",
    "for tensor in [scalar, vector, matrix]:\n",
    "    print(tensor.device) # Standard ist \"cpu\"\n",
    "\n",
    "tensor4 = torch.tensor([42, 42, 42], dtype=torch.float64, device=\"cuda\") # implicit index is the \"current device index\"\n",
    "print(tensor4.device)\n",
    "\n",
    "print(torch.device(\"cuda\", 1))\n",
    "\n",
    "#Geräte (devices) können sogar als Kontextmanager!! verwendet werden, d.h.\n",
    "\n",
    "with torch.device(0):\n",
    "    t = torch.tensor([1,2,3])\n",
    "print(t.device)\n",
    "\n",
    "# ja, das habe ich noch nicht so ganz verstanden!!\n",
    "\n",
    "# es gibt auch das Modul `torch.cuda`\n",
    "\n",
    "#man kann wohl auch tensoren erstellen, in dem man einfach nur dsa Gerät ändert, d.h. T = S.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8352fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.strided\n",
      "torch.strided\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "for tensor in [scalar, vector, matrix]:\n",
    "    print(tensor.layout) # Standard ist torch.strided (dichte Tensoren, s. \"strided arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09607c7",
   "metadata": {},
   "source": [
    "### Einschub: Gradientenberechnung\n",
    "Das Attribut `torch.Tensor.requires_grad` kann auf `True` gesetzt werden, um ...\n",
    "\n",
    "\n",
    "Der Datentyp `torch.no_grad` und `torch.enable_grad` bzw. die Klasse kann auch als Kontextmanager verwendet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9d1ff0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tensorGrad = torch.tensor([3, 7, 2 ,3], dtype=torch.float64, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "print(tensorGrad.requires_grad)\n",
    "\n",
    "#...\n",
    "\n",
    "with torch.no_grad():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75d9b1",
   "metadata": {},
   "source": [
    "## Tensoren erstellen\n",
    "\n",
    "Es gibt auch noch eine Menge weiterer Funktionen, um Tensoren zu erstellen. Neben `torch.tensor()` gibt es verschiedene \"Schablonen\", mit denen man vorgerftigte Tensoren nutzen kann:\n",
    "* `torch.empty(size)` - Tensor mit unitialisierten Daten\n",
    "* `torch.ones(size)`\n",
    "* `torch.ones_like(input: torch.Tensor)`\n",
    "* `torch.zeros(size)`\n",
    "* `torch.zeros_like(input: torch.Tensor)`\n",
    "(verwendet man u.a. fürs Masking)\n",
    "\n",
    "> Die \"_like\"-Funktionen benötigen einen `torch.Tensor` als Eingabe und nutzen die gleiche Form (`torch.Tensor.shape`), Dimension (`torch.Tensor.ndim`), Datentyp der Datenwerte (`torch.Tensor.dtype`) und Gerät (`torch.Tensor.device`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad01789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.4359e-25, 0.0000e+00, 1.3890e-05],\n",
      "        [0.0000e+00, 1.1461e-02, 4.2273e-41],\n",
      "        [1.3593e-43, 0.0000e+00, 9.4778e-30]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[12.23, 3.2], [3.4, 5.6]])\n",
    "\n",
    "a = torch.empty(3,3) # wie wird das initialisiert??\n",
    "b = torch.ones(10,2)\n",
    "c = torch.ones_like(input=z)\n",
    "d = torch.zeros(100)\n",
    "e = torch.zeros_like(input=X)\n",
    "for tensor in [a, b, c, d, e]:\n",
    "    print(tensor)\n",
    "\n",
    "# was ist mit torch.empty_like, torch.full, torch.full_like, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b51d79",
   "metadata": {},
   "source": [
    "Neben den obigen Funktionen bietet PyTorch auch noch folgende Muster: für 1-D und 2-D Tensoren!!!\n",
    "* `torch.eye(n:int, m:int)` - 2-D Tensor mit Einsen auf der Diagonalen und Nullen überall sonst (Motivation des Bezeichners??)\n",
    "* `torch.arange(start, end, steps:int)` - 1-D Tensor der Größe $\\lceil \\frac{end-start}{step}\\rceil$ dessen Werte aus dem Intervall $[start, end)$ stamen mit einer gemeinsamen Differenzstufe beginnend bei `start` (Ende ist exklusiv)\n",
    "* `torch.linspace()` - 1-D Tensor der Größe `steps` dessen Werte von Anfang bis Ende inklusiv gleichmäßig verteilt sind (linearly spaced)\n",
    "\n",
    "> `torch.range` ist deprecated und sollte eher nicht verwendet werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.]])\n",
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([1.0000, 1.7000, 2.4000, 3.1000, 3.8000, 4.5000, 5.2000, 5.9000, 6.6000,\n",
      "        7.3000, 8.0000, 8.7000, 9.4000])\n",
      "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])\n",
      "tensor([-10.,  -5.,   0.,   5.,  10.])\n",
      "tensor([-10.,  -5.,   0.,   5.,  10.])\n",
      "tensor([-10.,  10.])\n"
     ]
    }
   ],
   "source": [
    "f = torch.eye(7,13)\n",
    "g = torch.eye(6) #default: m=n (Einheitsmatrix!)\n",
    "h = torch.arange(6) #default: start=0, steps=1; ein Vektor mit n Werten von 0 bis n-1\n",
    "i = torch.arange(1,10) #default: steps=1; ein Vektor mit end-start Werten\n",
    "j = torch.arange(1, 10, 0.7) # kann es hier nicht Rundungsfehler geben????, integer dtypes!!!!, oder epsilon verwenden!\n",
    "k = torch.linspace(3, 10, steps=5)\n",
    "l = torch.linspace(-10, 10, steps=5)\n",
    "m = torch.linspace(start=-10, end=10, steps=5)\n",
    "n = torch.linspace(start=-10, end=10, steps=2)\n",
    "\n",
    "# es gibt auch noch torch.logspace\n",
    "\n",
    "for tensor in [f, g, h, i, j, k, l, m, n]:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49209fc",
   "metadata": {},
   "source": [
    "### Bringen wir etwas mehr Zufall hinein\n",
    "* `rand(size)` - zufällige Zahlen aus einer Gleichverteilung auf dem Intervall $[0,1)$, d.h. $x \\sim \\mathcal{U}(0,1)$\n",
    "* `rand_like(input: torch.Tensor)` - $x \\sim \\mathcal{U}(0,1)$ auf dem Intervall $[0,1)$,\n",
    "* `randint(low: int, high:int, size)` - $x \\sim \\mathcal{U}(low,high)$ auf dem Intervall $[low,high)$\n",
    "* `randint_like(input: torch.Tensor, low:int, high:int)` - $x \\sim \\mathcal{U}(low,high)$ auf dem Intervall $[low,high)$\n",
    "* `randn(size)`- zufällige Zahlen aus einer Normalverteilung mit Erwartungswert $\\mu = 0$ und Varianz $\\sigma^2 = 1$, d.h. $x \\sim \\mathcal{N}(0,1)$\n",
    "(Standardnormalverteilung)\n",
    "* `randn_like(input: torch.Tensor)` - $x \\sim \\mathcal{N}(0,1)$\n",
    "* `normal()` - $x \\sim \\mathcal{N}(\\mu,\\sigma^2)$ (ist überladen!!!!!!!!!!)\n",
    "\n",
    "(Anmerkung: Sowas gibt es auch in NumPy, oder durch die Python-Bibliothek `random`)\n",
    "\n",
    "(Wie genau sind diese Funktionen implementiert??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b438c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7931, 0.3076, 0.0462, 0.2549])\n",
      "tensor([[0.1223, 0.0152, 0.0996, 0.7202],\n",
      "        [0.1012, 0.6805, 0.4431, 0.7649],\n",
      "        [0.7047, 0.0071, 0.8302, 0.4833]])\n",
      "tensor([[0.7547, 0.6261],\n",
      "        [0.5027, 0.2383]])\n",
      "tensor([1, 2, 3, 6, 3, 4, 9, 7, 2, 5])\n",
      "tensor([[6., 9.],\n",
      "        [4., 6.]])\n",
      "tensor([ 0.2086,  0.2102,  0.8459, -0.4496])\n",
      "tensor([[-1.5765,  0.7461,  1.1739,  0.5525],\n",
      "        [-0.7298,  0.7917, -1.4118, -0.9586]])\n",
      "tensor([[ 0.3828,  1.0361],\n",
      "        [ 1.0880, -0.1976]])\n",
      "tensor([ 2.3984,  1.3204,  2.2680,  3.8678,  4.1040,  6.2380,  7.1658,  7.7369,\n",
      "         8.7722, 10.0545])\n",
      "tensor([[ 2.6723,  1.4971, -0.9575,  5.2608]])\n"
     ]
    }
   ],
   "source": [
    "o = torch.rand(4)\n",
    "p = torch.rand(3,4) #ndim=2, shape=torch.Size([3,4])\n",
    "q = torch.rand_like(X)\n",
    "r = torch.randint(1, 10, (10,))\n",
    "s = torch.randint_like(X, 1, 10)\n",
    "t = torch.randn(4)\n",
    "u = torch.randn(2,4) #ndim=2, shape=torch.Size([2,4])\n",
    "v = torch.randn_like(X)\n",
    "w = torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n",
    "x = torch.normal(2, 3, size=(1, 4))\n",
    "\n",
    "for tensor in [o, p, q, r, s, t, u, v, w, x]:\n",
    "    print(tensor)\n",
    "\n",
    "\n",
    "#Es gibt auch noch torch.bernoulli, torch.multinomial, torch.poisson, torch.randperm\n",
    "# Sollte man grundsätzlich die Argumente bei Python-funktionen hinschreiben, oder nicht?\n",
    "# Wie genau wird hier gesampelt??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4d473",
   "metadata": {},
   "source": [
    "Für Numpy-Integration gibt es dann auch noch:\n",
    "* `torch.from_numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3512287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ar = np.array([1,2,3])\n",
    "\n",
    "tensor1 = torch.from_numpy(ar)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ced4ab",
   "metadata": {},
   "source": [
    "## Operationen auf Tensoren\n",
    "> Wir schauen uns eigentlich nur die Methoden der Klasse `torch.Tensor` an und allgemeine Operationen auf Tensoren.\n",
    "\n",
    "Seien die Tensoren $T, S \\in \\mathbb{R}^{k_1 \\times k_2 \\times \\ldots \\times k_i}$ und $c \\in \\mathbb{R}$. Ähnlich wie bei Matrixoperationen können wir auch hier die grundlegenden arithmetischen Operationen auf die Tensoren anwenden (sofern die Elemente eines Tensors Elemente einer algebraischen Struktur sind), u.a. \n",
    "- Skalarmultiplikation: $c \\cdot T$ (in PyTorch: `c * T`)\n",
    "- transponieren: $T^T$ (in PyTorch: `T.T`) (existiert nicht für 1D-Tensoren, muss ich nochmal durchdenken für höherdimensionale Tensoren)\n",
    "(scheint ein Tensor der Größe $k_1 \\times k_2 \\times \\ldots \\times k_i$ zu einem Tensor der Größe $k_i \\times k_{i-1} \\times \\ldots \\times k_1$ zu machen)\n",
    "(was genau macht transponieren eigentlich????)\n",
    "\n",
    "\n",
    "Sofern die Tensoren gleich groß sind: (und `torch.dtype` und `torch.device` gleich sind)\n",
    "- Tensoraddition: $T + S$ (in PyTorch: `T + S`, `__add__`) (assoziativ, kommutativ, distributiv mit dem Hadamard-Produkt)\n",
    "- inverses Element der Tensoraddition (also Subtraktion): $T - S$ (in PyTorch: `T - S`, `__sub__`) (i.d.R. linksassoziativ)\n",
    "- elementweise Tensormultiplikation (sog. Hadamard-Produkt): $T \\circ S$ (in PyTorch: `T * S`, `__mul__`) (assoziativ, distributiv mit der Tensoraddition) (ist kommutativ in PyTorch)\n",
    "(Es gibt viele mögliche Varianten, eine Tensormuliplikation zu definieren. Hier nutzen wir das Hadamard-Produkt übertragen auf Tensoren. Das ist keine Multiplikation im algebraischem Sinn!)\n",
    "- inverses Element des Hadamard-Produktes: $T / S$ (in PyTorch: `T / S`, `__truediv__`) (i.d.R. linksassoziativ) (liefert inf bei Teilung durch 0!!; castet automatisch zu float32)\n",
    "\n",
    "> Wir überladen hier bereits bekannte binäre Operationen aus Python.\n",
    "\n",
    "\n",
    "@ -Operation!, `__matmul__` (min. 1D, nimmt keine Zahlen an!!!; Tensoren müssen passende Größen haben!)\n",
    "(Matrixmultiplikation; spezielle Implementation; erlaubt kein dyadisches Produkt; strukturelle Informationen bei 1D-Tensoren werden ignoriert, d.h. hier existiert kein Zeilen- oder Spaltenvektor)\n",
    "(bei 3D-Tensoren funktioniert das auch -> das muss ich noch mal überdenken)\n",
    "(Spezialfälle der Matrixmultiplikation werden quasi verallgemeinert!!!!!)\n",
    "(muss ich noch für höhere Dimensionen durchdenken, z.B. 3D-Tensor mal 3D-Tensor; oder Vektor mal 3D-Tensor, Vektor mal 4D-Tensor, etc.)\n",
    "\n",
    "\n",
    "T + Skalar\n",
    "T - Skalar\n",
    "T / Skalar\n",
    "T mod Skalar\n",
    "\n",
    "\n",
    "- Modulo-Operation: $T \\mod S$ (in PyTorch: `T % S`, `__mod__`) (i.d.R. linksassoziativ) (liefert `ZeroDivisionError`, wenn `S == 0`) (in der linearen Algebra definiert man glaube ich nicht die $mod$-Operation auf Tensoren)\n",
    "\n",
    "> Wir überladen hier bereits bekannte binäre Operationen aus Python.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Präzedenz in PyTorch??\n",
    "\n",
    "\n",
    "torch.multiply\n",
    "torch.mul\n",
    "torch.add\n",
    "torch.matmul\n",
    "torch.mm\n",
    "\n",
    "\n",
    "(hier hätte ich gerne mehr theoretisches Wissen bzw. Wissen darüber, wie Tensoren mathematisch verstanden werden!)\n",
    "\n",
    "\n",
    "\n",
    "ich müsste mir nochmal gesonert 3D-Tensoren, 4D-Tensoren, 5D-Tensoren, usw. anschauen!\n",
    "\n",
    "bei Matrizen gebe ich ja durch die Array-Definition die strukturellen Informationen mit, bei Vektoren nicht\n",
    "\n",
    "Werden die 1D-Tensoren nun als Spaltenvektoren verstanden???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ff3e6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "in-place operations: indicated by _, e.g. add_\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4ae4f",
   "metadata": {},
   "source": [
    "flatten() -> ist quasi Vektorisierung, d.h. man nutzt die Isomorphie von Tensorräumen zu $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e8087",
   "metadata": {},
   "source": [
    "bereits bekannt: size(), type(), to(), numpy()\n",
    "\n",
    "\n",
    " \n",
    "item() (funktioniert nur für einelementige Tensoren!)\n",
    "\n",
    "\n",
    "\n",
    "Indexing: similar to python!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Slicing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Joining\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Mutating\n",
    "\n",
    "\n",
    "cat(), reshape(), stack(), squeeze(), unsqueeze(), transpose(), permute()\n",
    "\n",
    "\n",
    "mathematische Operationen: abs(), mul(), add(), matmul(), pow(), sum(), mean()\n",
    "\n",
    "\n",
    "\n",
    "einsum()??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd76529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5, 6, 4, 2],\n",
      "         [7, 8, 1, 7]],\n",
      "\n",
      "        [[7, 6, 3, 3],\n",
      "         [7, 8, 1, 2]],\n",
      "\n",
      "        [[7, 6, 3, 3],\n",
      "         [7, 8, 6, 5]]])\n",
      "tensor([[[5, 6, 4, 2],\n",
      "         [7, 8, 1, 7]],\n",
      "\n",
      "        [[7, 6, 3, 3],\n",
      "         [7, 8, 1, 2]],\n",
      "\n",
      "        [[7, 6, 3, 3],\n",
      "         [7, 8, 6, 5]]])\n",
      "tensor([[[5, 7, 7],\n",
      "         [7, 7, 7]],\n",
      "\n",
      "        [[6, 6, 6],\n",
      "         [8, 8, 8]],\n",
      "\n",
      "        [[4, 3, 3],\n",
      "         [1, 1, 6]],\n",
      "\n",
      "        [[2, 3, 3],\n",
      "         [7, 2, 5]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "S = torch.tensor([1,2])\n",
    "print(T)\n",
    "T = torch.tensor([[[5,6,4, 2], [7,8,1,7]], [[7,6,3,3], [7,8,1,2]], [[7,6,3,3], [7,8,6,5]]])\n",
    "R = torch.tensor([3,2,4])\n",
    "U = torch.ones(2,3,4,5,6)\n",
    "print(T)\n",
    "\n",
    "W = T.T\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f59a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7abd35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae643ddd",
   "metadata": {},
   "source": [
    "# GPGU with PyTorch\n",
    "- torch.cuda\n",
    "- torch.cuda.is_avalaible()\n",
    "- torch.cuda.device_count()\n",
    "- torch.Tensor.to()\n",
    "- torch.Tensor.cpu()\n",
    "- torch.accelerator\n",
    "- torch.accelerator.current_accelerator\n",
    "- torch.accelerator.type\n",
    "- torch.accelerator.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
