{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02715975",
   "metadata": {},
   "source": [
    "# Data in PyTorch\n",
    "Wenn man gewisse Modelle (u.a. neuronale Netze) trainieren und testen will, benötigt man natürlich einen gewissen Input. Dieser repräsentiert i.d.R. einen Datensatz.\n",
    "Für bessere Leserlichkeit und Modularität ist es hilfreich, wenn der Datensatz selbst und die Trainingsdaten entkoppelt sind.\n",
    "In PyTorch ist dies durch verschiedene Klassen (Datentypen) im Modul `torch.utils.data` möglich. (ähnlich wie torch.dtype, torch.device, torch.layout, torch.Size, ..., )\n",
    "- `torch.utils.data.Dataset` - Klasse, mit der man eigene Datensätze erstellen kann\n",
    "- `torch.utils.data.DataLoader` - ein Python-Iterator über `torch.utils.data.Dataset`-Datensätze (Wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf288cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99c6cf",
   "metadata": {},
   "source": [
    "Nun gibt es im ML-Bereich verschiedene Arten von Daten. Wie bereits erwähnt gibt es verschiedene Module, die verschiedene Arten von Daten bereitstellen, u.a.\n",
    "- Bilder: `torchvision`\n",
    "- Texte: `torchtext`\n",
    "- Sprache & Audio: `torchaudio`\n",
    "\n",
    "Das Modul `torchvision.datasets` stellt verschiedene Bilderdatensätze bereit.\n",
    "Das Modul `torchvision.transforms` stellt ?? bereit.\n",
    "- `torchvision.transforms.ToTensor`\n",
    "- `torchvision.transforms.Compose`\n",
    "- `torchvision.transforms.Normalize`\n",
    "- `torchvision.transforms.Lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a6fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8617badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b2a2551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_example = datasets.FashionMNIST()\n",
    "type(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66b5d2",
   "metadata": {},
   "source": [
    "## Eigene Datensätze mit `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f27992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
